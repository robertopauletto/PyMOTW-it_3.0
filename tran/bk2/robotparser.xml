<?xml version="1.0" encoding="Latin-1"?>
<categoria>formati di file </categoria><!-- # tag <descrizione> contiene le voci per la tabella di riepilogo iniziale -->
<!-- # tag <titolo_1> titolo principale -->
<!-- tag <testo_normale> contiene il testo normale dell'articolo -->
<!-- tag <titolo_2> contiene il testo per l'intestazione di un paragrafo -->
<!-- tag <py_code> contiene il testo che rappresenta delle istruzioni python -->
<!-- tag <py_output> contiene il testo che rappresenta l'output di uno script python -->
<!-- tag <vedi_anche> contiene il testo che rappresentano i riferimenti esterni -->
<!-- tag <lista> ogni riga all'interno del tag rappresenta una riga di una lista non ordinata'-->
<documento_tradotto>
<titolo_1>
robotparser - Controlla l'accesso degli spider internet
</titolo_1>
<descrizione>
Elabora il file robots.txt usato per controllare gli spider internet
2.1.3 e successiva
</descrizione>
<testo_normale>
Il modulo <strong>robotparser</strong> implementa un parser per il formato del file ${sbk}robots.txt${ebk}, inclusa una semplice funzione per controllare se un dato user agent può avere accesso ad una risorsa. E' inteso per l'uso con spider "ben-educati" od altre applicazioni <a href='http://it.wikipedia.org/wiki/Crawler'>crawler</a>  che devono essere limitate od in altromodo ristrette.
</testo_normale>
<note>
Il modulo <strong>robotparser</strong> è stato rinominato <strong>urllib.robotparser</strong> in Python 3.0. Il codice esistente che usa <strong>robotparser</strong> può essere aggiornato usando 2to3.
</note>
<titolo_2>
robots.txt    
</titolo_2>
<testo_normale>
Il formato del file ${sbk}robots.txt${ebk} è un formato di testo con un semplice sistema di controllo degli accessi per i programmi che accedono automaticamente alle risorse web ("spider", "crawler", ecc.). Il file è composto da record che specificano l'identificatore dell' l'user agent per il programma, seguito da un elenco di URL (o prefissi di URL) ai quali l'agente non dovrebbe accedere.
</testo_normale>
<testo_normale>
Questo è il file ${sbk}robots.txt${ebk} per ${sbk}http://www.doughellmann.com/${ebk}
</testo_normale>
<py_output>
User-agent: *
Disallow: /admin/
Disallow: /downloads/
Disallow: /media/
Disallow: /static/
Disallow: /codehosting/
</py_output>
<testo_normale>
Previene l'accesso a parti rilevanti del sito che potrebbero sovraccaricare il server se un motore di ricerca tentasse di indicizzarle. Per una serie di esempi più completi fare riferimento a <a href='http://www.robotstxt.org/orig.html'>The Web Robots Page.</a>
</testo_normale>
<titolo_2>
Un Semplice Esempio    
</titolo_2>
<testo_normale>
Usando i dati di cui sopra, un semplice crawler può verificare se gli è consentito scaricare una pagina usando il metodo di ${sbk}RobotFileParser${ebk} ${sbk}can_fetch()${ebk}
</testo_normale>
<py_code>
import robotparser
import urlparse

AGENT_NAME = 'PyMOTW'
URL_BASE = 'http://www.doughellmann.com/'
parser = robotparser.RobotFileParser()
parser.set_url(urlparse.urljoin(URL_BASE, 'robots.txt'))
parser.read()

PATHS = [
    '/',
    '/PyMOTW/',
    '/admin/',
    '/downloads/PyMOTW-1.92.tar.gz',
    ]

for path in PATHS:
    print '%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path)
    url = urlparse.urljoin(URL_BASE, path)
    print '%6s : %s' % (parser.can_fetch(AGENT_NAME, url), url)
    print
</py_code>
<testo_normale>
Il parametro URL per ${sbk}can_fetch()${ebk} può essere un percorso relativo alla radice del sito, oppure un URL completo.
</testo_normale>
<py_output>
$ python robotparser_simple.py
  True : /
  True : http://www.doughellmann.com/

  True : /PyMOTW/
  True : http://www.doughellmann.com/PyMOTW/

 False : /admin/
 False : http://www.doughellmann.com/admin/

 False : /downloads/PyMOTW-1.92.tar.gz
 False : http://www.doughellmann.com/downloads/PyMOTW-1.92.tar.gz
</py_output>
<titolo_2>
Spider Longevi    
</titolo_2>
<testo_normale>
Una applicazione che impiega molto tempo per elaborare le risorse che scarica o che è limitata a mettersi in pausa tra i download potrebbe volere verificare periodicamente se esistono nuovi file ${sbk}robots.txt${ebk} in base all'età del contenuto che ha già scaricato. L'etàa non è gestita automaticamente, ma ci sono comodi metodi per tenerne facilmente traccia.
</testo_normale>
<py_code>
import robotparser
import time
import urlparse

AGENT_NAME = 'PyMOTW'
parser = robotparser.RobotFileParser()
# Si usa la copia locale
parser.set_url('robots.txt')
parser.read()
parser.modified()

PATHS = [
    '/',
    '/PyMOTW/',
    '/admin/',
    '/downloads/PyMOTW-1.92.tar.gz',
    ]

for n, path in enumerate(PATHS):
    print
    age = int(time.time() - parser.mtime())
    print 'age:', age,
    if age > 1:
        print 're-reading robots.txt'
        parser.read()
        parser.modified()
    else:
        print
    print '%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path)
    # Simulazione di un ritardo nell'elaborazione
    time.sleep(1)
</py_code>
<testo_normale>
Questo esempio particolarmente semplice scarica un nuovo file ${sbk}robots.txt${ebk} se quello esistente è più vecchio di più di 1 secondo.
</testo_normale>
<py_output>
$ python robotparser_longlived.py

age: 0
  True : /

age: 1
  True : /PyMOTW/

age: 2 rilettura di robots.txt
 False : /admin/

age: 1
 False : /downloads/PyMOTW-1.92.tar.gz
</py_output>
<testo_normale>
Una versione più "elegante" di questa applicazione potrebbe richiedere l'ora di modifica per il file prima di scaricarlo interamente. D'altro canto, i file ${sbk}robots.txt${ebk} sono in genere piuttosto piccoli, quindi non costa più di tanto recuperare di nuovo l'intero documento.
</testo_normale>
<vedi_anche>
http://docs.python.org/library/robotparser.html|robotparser|La documentazione della libreria standard per questo modulo.
http://www.robotstxt.org/orig.html|The Web Robots Page|Descrizione del formato di robots.txt
</vedi_anche>
</documento_tradotto>
